# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rEA7R2Y6wi6wGq2yvr03mG1GKkRXal3x
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/content/laptop.csv')

# Inspect the first few rows
print(df.head())

# Check for missing values
print(df.isnull().sum())

# Get a concise summary of the DataFrame
print(df.info())

# Rename columns if necessary (based on your description)
df.columns = ['Unnamed: 0.1', 'Unnamed: 0', 'Company', 'TypeName', 'Inches', 'ScreenResolution', 'Cpu', 'Ram', 'Memory', 'Gpu', 'OpSys', 'Weight', 'Price']

# Drop redundant columns
df = df.drop(columns=['Unnamed: 0.1'])

# Drop rows with missing 'Price'
df = df.dropna(subset=['Price'])

# Drop the 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Summary statistics
print(df.describe())

# Visualize price by TypeName before one-hot encoding
plt.figure(figsize=(12, 8))
sns.boxplot(x='TypeName', y='Price', data=df)
plt.xticks(rotation=90)
plt.title('Price by Laptop Type')
plt.show()

# Distribution of prices
plt.figure(figsize=(10, 6))
sns.histplot(df['Price'], bins=30, kde=True)
plt.title('Distribution of Laptop Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Replace or handle non-numeric values
df['Inches'] = pd.to_numeric(df['Inches'], errors='coerce')
df['Weight'] = pd.to_numeric(df['Weight'].str.replace('kg', '', regex=False), errors='coerce')
df['Ram'] = df['Ram'].str.replace('GB', '').astype(int)

# Handle missing values
df['Inches'].fillna(df['Inches'].median(), inplace=True)
df['Weight'].fillna(df['Weight'].median(), inplace=True)

import numpy as np
import matplotlib.pyplot as plt

# Assume rf is your RandomForestRegressor model
feature_importances = rf.feature_importances_

# Get categorical features related to 'Company'
company_features = [col for col in X.columns if col.startswith('Company_')]

# Get the indices of these features
company_indices = [X.columns.get_loc(col) for col in company_features]

# Extract importances for these features
company_importances = np.array([feature_importances[idx] for idx in company_indices])

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(company_features, company_importances)
plt.xlabel('Feature Importance')
plt.title('Impact of Company on Laptop Prices')
plt.show()

# One-hot encode categorical variables
df = pd.get_dummies(df, columns=['Company', 'TypeName', 'ScreenResolution', 'Cpu', 'Memory', 'Gpu', 'OpSys'], drop_first=True)

# Check data types and columns after encoding
print(df.dtypes)
print(df.columns)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['Weight', 'Inches']] = scaler.fit_transform(df[['Weight', 'Inches']])

from sklearn.model_selection import train_test_split
X = df.drop('Price', axis=1)
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor

lr = LinearRegression()
rf = RandomForestRegressor()
gb = GradientBoostingRegressor()

lr.fit(X_train, y_train)
rf.fit(X_train, y_train)
gb.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error

def evaluate_model(model):
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    return mae, mse

for model in [lr, rf, gb]:
    mae, mse = evaluate_model(model)
    print(f"Model: {model.__class__.__name__}, MAE: {mae}, MSE: {mse}")

from sklearn.model_selection import GridSearchCV

param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3)
grid_search.fit(X_train, y_train)
print(f"Best parameters: {grid_search.best_params_}")

# Feature importance
feature_importances = rf.feature_importances_
sorted_idx = feature_importances.argsort()
plt.barh(X.columns[sorted_idx], feature_importances[sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance of RandomForest Model')
plt.show()

# Relationship between numerical features and price
sns.pairplot(df[['Price', 'Inches', 'Ram', 'Weight']], diag_kind='kde')
plt.show()

# Filter for lesser-known brands (example brands)
less_known_brands = df[df['Company'].isin(['Vero', 'Chuwi', 'Toshiba'])]

# Split and evaluate the model on this subset
X_less_known = less_known_brands.drop('Price', axis=1)
y_less_known = less_known_brands['Price']
X_train_less, X_test_less, y_train_less, y_test_less = train_test_split(X_less_known, y_less_known, test_size=0.2, random_state=42)

# Evaluate model performance on this subset
mae, mse = evaluate_model(rf)  # or other models
print(f"Performance on lesser-known brands: MAE: {mae}, MSE: {mse}")

# Define high-end and budget laptops (example thresholds)
high_end = df[df['Price'] > 35000]
budget = df[df['Price'] <= 35000]

# Evaluate model on these subsets
X_high_end = high_end.drop('Price', axis=1)
y_high_end = high_end['Price']
X_budget = budget.drop('Price', axis=1)
y_budget = budget['Price']

mae_high_end, mse_high_end = evaluate_model(rf)  # or other models
mae_budget, mse_budget = evaluate_model(rf)  # or other models

print(f"Performance on high-end laptops: MAE: {mae_high_end}, MSE: {mse_high_end}")
print(f"Performance on budget laptops: MAE: {mae_budget}, MSE: {mse_budget}")